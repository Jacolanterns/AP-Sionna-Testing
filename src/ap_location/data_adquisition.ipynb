{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import hashlib\n",
    "import csv\n",
    "from influxdb_client import InfluxDBClient\n",
    "from influxdb_client.client.warnings import MissingPivotFunction\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import glob\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.simplefilter(\"ignore\", MissingPivotFunction)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "GET_DATA_FROM_FETCH = True \n",
    "GET_DATA_FROM_INFLUX = False\n",
    "INFLUX_BUCKET = \"aruba\"\n",
    "OUTPUT_FILE = 'ap_data_all_floors.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InfluxDB query\n",
    "INFLUX_QUERY = '''from(bucket: \"aruba\")\n",
    "  |> range(start: 2024-04-15T07:00:00Z, stop: 2024-07-30T12:00:00Z)\n",
    "  |> filter(fn: (r) => r[\"_measurement\"] == \"adjacent-ap\")\n",
    "  |> filter(fn: (r) => r[\"building\"] == \"D1\")\n",
    "  |> filter(fn: (r) => r[\"ap-type\"] == \"valid\")\n",
    "  |> filter(fn: (r) => r[\"_field\"] == \"curr-rssi\" or r[\"_field\"] == \"crawling-duration (ms)\")\n",
    "  |> aggregateWindow(every: 15m, fn: mean, createEmpty: false)\n",
    "  |> yield(name: \"mean\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_influx_data(query):\n",
    "    \"\"\"Fetch data from InfluxDB.\"\"\"\n",
    "    token = os.environ.get(\"INFLUXDB_TOKEN\")\n",
    "    INFLUX_URL = os.environ.get(\"INFLUX_URL\")\n",
    "    INFLUX_ORG = os.environ.get(\"INFLUX_ORG\")\n",
    "    if not token:\n",
    "        raise ValueError(\"INFLUXDB_TOKEN not found in environment variables\")\n",
    "\n",
    "    client = InfluxDBClient(url=INFLUX_URL, token=token, org=INFLUX_ORG)\n",
    "    query_api = client.query_api()\n",
    "    \n",
    "    data = query_api.query_data_frame(org=INFLUX_ORG, query=query)\n",
    "    print(f\"Data fetched: {data.shape}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sha256(bss):\n",
    "    return hashlib.sha256(bss.encode()).hexdigest()\n",
    "\n",
    "def process_data(data, floor_filters):\n",
    "    \"\"\"Process the raw data with optimizations.\"\"\"\n",
    "    columns = ['_time', 'essid', 'adjacent-ap-bssid', 'band', 'monitoring-ap', '_value']\n",
    "    data = data[columns]\n",
    "    \n",
    "    # Use boolean indexing for filtering\n",
    "    mask = (data['band'] == '2.4GHz') & \\\n",
    "           (data['monitoring-ap'].notna()) & \\\n",
    "           (data['monitoring-ap'].str.contains(floor_filters, regex=True))\n",
    "    data = data[mask]\n",
    "    \n",
    "    mask_64 = data[\"adjacent-ap-bssid\"].str.len() == 64\n",
    "    data.loc[~mask_64, \"adjacent-ap-bssid\"] = data.loc[~mask_64, \"adjacent-ap-bssid\"].apply(compute_sha256)\n",
    "    \n",
    "    # Pivot the data\n",
    "    pivot_table = pd.pivot_table(data, \n",
    "                                 values='_value', \n",
    "                                 index=['_time', 'essid', 'adjacent-ap-bssid', 'band'],\n",
    "                                 columns='monitoring-ap')\n",
    "    \n",
    "    result = pivot_table.reset_index()\n",
    "    result.columns = ['_time', 'essid', 'bssid', 'band'] + ['rssi_' + str(col) for col in pivot_table.columns]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sha256(value):\n",
    "    \"\"\"Compute SHA256 hash of a string.\"\"\"\n",
    "    return hashlib.sha256(value.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def translate_bssids(df, floor_filters):\n",
    "    with open('../data/ap_bss_table.json', 'r') as file:\n",
    "        bss_data = json.load(file)\n",
    "\n",
    "    bss_df = pd.DataFrame(bss_data[\"Aruba AP BSS Table\"])\n",
    "    bss_df['bssid'] = bss_df['bss'].apply(compute_sha256)\n",
    "\n",
    "    merged_df = pd.merge(df, bss_df[['bssid', 'ap name']], on='bssid', how='left')\n",
    "    merged_df.rename(columns={'ap name': 'ap_name'}, inplace=True)\n",
    "    merged_df = merged_df[merged_df['ap_name'].notna()]\n",
    "    merged_df = merged_df[merged_df['ap_name'].str.contains(floor_filters, regex=True)]\n",
    "\n",
    "    return merged_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_coordinate_file(file_path):\n",
    "    \"\"\"Read AP coordinate data from a CSV file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return {row[0]: (float(row[1]), float(row[2]), float(row[3])) for row in csv.reader(file)}\n",
    "\n",
    "def add_coordinates(df, ap_data):\n",
    "    \"\"\"Add x, y, and z coordinates to the dataframe.\"\"\"\n",
    "    df[['x', 'y', 'z']] = df['ap_name'].map(ap_data).apply(pd.Series)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_files = sorted(glob.glob('../data/[2-9]f.csv') + glob.glob('../data/1[0-3]f.csv'))\n",
    "floor_numbers = [file.split('/')[-1].split('f')[0] for file in floor_files]\n",
    "floor_filters = '|'.join([f'D1_{floor}F' for floor in floor_numbers])\n",
    "\n",
    "ap_data = {}\n",
    "for file in floor_files:\n",
    "    ap_data.update(read_coordinate_file(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9106/3931779889.py:9: DtypeWarning: Columns (1,2,6,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_io,\n"
     ]
    }
   ],
   "source": [
    "if GET_DATA_FROM_FETCH:\n",
    "    # fetch using web api and js\n",
    "    with open('../data/aruba_07_15.json', 'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Use StringIO to create a file-like object from the string\n",
    "    data_io = io.StringIO(data)\n",
    "\n",
    "    df = pd.read_csv(data_io, \n",
    "                    comment='#',\n",
    "                    skiprows=3,  \n",
    "                    parse_dates=['_start', '_stop', '_time'], \n",
    "                    na_values=[''],  \n",
    "                    #nrows=50000,\n",
    "                    )\n",
    "else:\n",
    "    if GET_DATA_FROM_INFLUX:\n",
    "        df = get_influx_data(INFLUX_QUERY)    \n",
    "        filename = f\"data/aruba_data_{int(time.time())}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "    else:\n",
    "        df = pd.read_csv('data/aruba_2024-07-15T07:00:00Z_stop_2024-07-16T12:00:00Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = process_data(df, floor_filters)\n",
    "clean_df = translate_bssids(processed_data, floor_filters)\n",
    "final_df = add_coordinates(clean_df, ap_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70452, 70)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ap_data_all_floors.csv\n",
      "Number of different APs: 135\n",
      "Date range: 2024-07-14T03:00:00Z to 2024-07-15T02:48:00Z\n",
      "Total records: 70452\n",
      "Z-coordinate range: 32.4042 to 195.111\n",
      "Floors included: 10, 11, 12, 13, 2, 3, 4, 5, 6, 7, 8, 9\n"
     ]
    }
   ],
   "source": [
    "# Save the final dataframe\n",
    "final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Data saved to {OUTPUT_FILE}\")\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Number of different APs: {final_df['ap_name'].nunique()}\")\n",
    "print(f\"Date range: {final_df['_time'].min()} to {final_df['_time'].max()}\")\n",
    "print(f\"Total records: {len(final_df)}\")\n",
    "print(f\"Z-coordinate range: {final_df['z'].min()} to {final_df['z'].max()}\")\n",
    "print(f\"Floors included: {', '.join(floor_numbers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pai",
   "language": "python",
   "name": "pai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
