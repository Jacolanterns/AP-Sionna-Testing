{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AP location prediction \n",
    "**Following the methods explored in the `ap_coordinates_prediction_review.ipynb`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "import optuna\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the path to the main root of the project\n",
    "# This is the folder that contains the 'ap_data' directory\n",
    "project_root = '/home/sionna/Documents/GitTest2/AP-Sionna-Testing' \n",
    "\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Print the current working directory to confirm it's correct\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a path relative to your main project folder\n",
    "path = 'ap_data/ap_data_all_floors.csv'\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max valid RSSI to 200\n",
    "data = data.dropna(subset=[data.columns[-1]])\n",
    "data.replace({np.nan: 200}, inplace=True)\n",
    "data.iloc[:, 4:-4] = data.iloc[:, 4:-4].clip(upper=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_floors = data['ap_name'].str.extract('(\\d+F)')[0].unique()\n",
    "num_floors = len(unique_floors)\n",
    "fig, axes = plt.subplots(1, num_floors, figsize=(20 * num_floors // 2, 8))\n",
    "\n",
    "for i, floor in enumerate(sorted(unique_floors)):\n",
    "    floor_data = data[data['ap_name'].str.contains(floor)]\n",
    "    axes[i].scatter(floor_data.iloc[:, -3], floor_data.iloc[:, -2])\n",
    "    axes[i].set_xlabel('X Position')\n",
    "    axes[i].set_ylabel('Y Position')\n",
    "    axes[i].set_title(f'Scatter Plot of Positions ({floor})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't enough data to cover all the access points (APs), so some floors appear empty due to the lack of data for all APs on those floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rounded_position'] = data.apply(lambda row: f\"{int(round(row.iloc[-3]))}_{int(round(row.iloc[-2]))}_{int(round(row.iloc[-1]))}\", axis=1)\n",
    "\n",
    "unique_floors = data['ap_name'].str.extract('(\\d+F)')[0].unique()\n",
    "num_floors = len(unique_floors)\n",
    "fig, axes = plt.subplots(1, num_floors, figsize=(20 * num_floors // 2, 8))\n",
    "\n",
    "for i, floor in enumerate(sorted(unique_floors)):\n",
    "    floor_data = data[data['ap_name'].str.contains(floor)]\n",
    "    position_counts = floor_data['rounded_position'].value_counts().sort_index()\n",
    "    \n",
    "    position_counts.plot(kind='bar', ax=axes[i])\n",
    "    axes[i].set_title(f'Frequency of Rounded X-Y Positions ({floor})')\n",
    "    axes[i].set_xlabel('Rounded X-Y Position')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].tick_params(axis='x', rotation=90)\n",
    "    \n",
    "    for j, v in enumerate(position_counts):\n",
    "        axes[i].text(j, v, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns we want to plot\n",
    "selected_data = data.iloc[:, 4:-4]\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 24))\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=selected_data, ax=ax1, whis=1.5)\n",
    "ax1.set_title('Box Plot of Column Distributions', fontsize=16)\n",
    "ax1.set_xlabel('Columns', fontsize=12)\n",
    "ax1.set_ylabel('Values', fontsize=12)\n",
    "ax1.tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Add strip plot to show individual points\n",
    "sns.stripplot(data=selected_data, ax=ax1, size=2, color=\".3\", linewidth=0)\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(data=selected_data, ax=ax2, cut=0)\n",
    "ax2.set_title('Violin Plot of Column Distributions', fontsize=16)\n",
    "ax2.set_xlabel('Columns', fontsize=12)\n",
    "ax2.set_ylabel('Values', fontsize=12)\n",
    "ax2.tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(selected_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_save_labels(data, column_name, encoder_file_name='label_encoder.pkl'):\n",
    "    # Initialize the LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    encoded_values = le.fit_transform(data[column_name])\n",
    "    \n",
    "    # Replace the original column with encoded values\n",
    "    data[column_name] = encoded_values\n",
    "    \n",
    "    # Save the encoder to a file\n",
    "    with open(encoder_file_name, 'wb') as file:\n",
    "        pickle.dump(le, file)\n",
    "    \n",
    "    print(f\"Encoded {column_name} and saved encoder to {encoder_file_name}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def decode_labels(data, column_name, encoder_file_name='label_encoder.pkl'):\n",
    "    # Load the encoder from the file\n",
    "    with open(encoder_file_name, 'rb') as file:\n",
    "        le = pickle.load(file)\n",
    "    \n",
    "    # Transform the encoded values back to original labels\n",
    "    decoded_values = le.inverse_transform(data[column_name])\n",
    "    \n",
    "    # Replace the encoded column with decoded values\n",
    "    data[column_name] = decoded_values\n",
    "    \n",
    "    print(f\"Decoded {column_name} using encoder from {encoder_file_name}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def decode_predictions(y_pred, encoder_file_name='label_encoder.pkl'):\n",
    "    # Load the encoder from the file\n",
    "    with open(encoder_file_name, 'rb') as file:\n",
    "        le = pickle.load(file)\n",
    "    \n",
    "    # Transform the encoded predictions back to original labels\n",
    "    decoded_predictions = le.inverse_transform(y_pred)\n",
    "    \n",
    "    print(f\"Decoded predictions using encoder from {encoder_file_name}\")\n",
    "    \n",
    "    return decoded_predictions\n",
    "\n",
    "## To see the mapping\n",
    "#with open('label_encoder.pkl', 'rb') as file:\n",
    "#    le = pickle.load(file)\n",
    "#    print(\"Label Mapping:\")\n",
    "#    for i, label in enumerate(le.classes_):\n",
    "#        print(f\"{label} -> {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.iloc[:, 4:-4]\n",
    "df = encode_and_save_labels(df, 'ap_name')\n",
    "df = df.rename(columns={'ap_name': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling using Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1] \n",
    "y = df.iloc[:, -1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaled_data = RobustScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(robust_scaled_data, edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of All Values (Excluding Last Column)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there overlapping points in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_data = df.copy()\n",
    "binned_data.iloc[:, :-1] = np.round(binned_data.iloc[:, :-1]) \n",
    "overlapping_points = binned_data.groupby(list(binned_data.columns[:-1]))['label'].nunique()\n",
    "overlapping_points = overlapping_points[overlapping_points > 1]\n",
    "overlapping_data = binned_data.set_index(list(binned_data.columns[:-1])).loc[overlapping_points.index]\n",
    "\n",
    "overlapping_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete overlapping data \n",
    "non_overlapping_mask = ~binned_data.set_index(list(binned_data.columns[:-1])).index.isin(overlapping_points.index)\n",
    "df = df[non_overlapping_mask]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model XGBClassifier With Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        if isinstance(features, np.ndarray): self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        else: self.features = torch.tensor(features.values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "dataset = CustomDataset(robust_scaled_data, y)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(robust_scaled_data, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.12, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.size, y_val.size, y_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost Classifier\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "# Train the model\n",
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_y_pred = decode_predictions(y_pred)\n",
    "decoded_y_test = decode_predictions(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ap_positions(data, y_test, y_pred):\n",
    "    unique_floors = data['ap_name'].str.extract('(\\d+F)')[0].unique()\n",
    "    num_floors = len(unique_floors)\n",
    "    fig, axes = plt.subplots(1, num_floors, figsize=(12 * num_floors // 2, 8))\n",
    "\n",
    "    ap_positions = dict(zip(data['ap_name'], zip(data['x'], data['y'])))\n",
    "\n",
    "    for i, floor in enumerate(sorted(unique_floors)):\n",
    "        floor_mask = [floor in ap for ap in y_test]\n",
    "        floor_y_test = y_test[floor_mask]\n",
    "        floor_y_pred = y_pred[floor_mask]\n",
    "\n",
    "        test_positions = [ap_positions[ap] for ap in floor_y_test]\n",
    "        pred_positions = [ap_positions[ap] for ap in floor_y_pred]\n",
    "\n",
    "        position_accuracy = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "        for true_pos, true_ap, pred_ap in zip(test_positions, floor_y_test, floor_y_pred):\n",
    "            position_accuracy[true_pos]['total'] += 1\n",
    "            if true_ap == pred_ap:\n",
    "                position_accuracy[true_pos]['correct'] += 1\n",
    "\n",
    "        position_percentage = {pos: (data['correct'] / data['total']) * 100 \n",
    "                               for pos, data in position_accuracy.items()}\n",
    "\n",
    "        x_test, y_test_coords = zip(*test_positions)\n",
    "        x_pred, y_pred_coords = zip(*pred_positions)\n",
    "\n",
    "        axes[i].scatter(x_test, y_test_coords, c='green', marker='o', s=200, alpha=0.5, label='Ground Truth')\n",
    "        scatter = axes[i].scatter(x_pred, y_pred_coords, \n",
    "                                  c=[position_percentage.get(pos, 0) for pos in test_positions],\n",
    "                                  cmap='RdYlGn', vmin=0, vmax=100, s=50, alpha=0.7)\n",
    "\n",
    "        correct_predictions = sum(true == pred for true, pred in zip(floor_y_test, floor_y_pred))\n",
    "        total_predictions = len(floor_y_test)\n",
    "        overall_accuracy = (correct_predictions / total_predictions) * 100\n",
    "\n",
    "        axes[i].set_title(f'AP Positions on {floor}\\nAccuracy: {overall_accuracy:.2f}%')\n",
    "        axes[i].set_xlabel('X Position')\n",
    "        axes[i].set_ylabel('Y Position')\n",
    "        axes[i].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    cbar = fig.colorbar(scatter, ax=axes.ravel().tolist())\n",
    "    cbar.set_label('Percentage of Correct Predictions')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ap_positions(data, decoded_y_test, decoded_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loc = data.iloc[:, 4:-1]\n",
    "d_loc = d_loc.drop(columns=['ap_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d_loc.iloc[:, :-3]\n",
    "y = d_loc.iloc[:, -3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_data_scaled = RobustScaler().fit_transform(X)\n",
    "dataset = CustomDataset(regression_data_scaled, y)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(robust_scaled_data, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.12, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "# Use MultiOutputRegressor to handle multiple targets\n",
    "multi_xgb = MultiOutputRegressor(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [100, 200, 300],\n",
    "    'estimator__max_depth': [3, 4, 5],\n",
    "    'estimator__learning_rate': [0.01, 0.1, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(multi_xgb, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "print(\"Best parameters found by grid search:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print all hyperparameters of the best model\n",
    "print(\"\\nAll hyperparameters of the best model:\")\n",
    "for i, estimator in enumerate(best_model.estimators_):\n",
    "    print(f\"\\nEstimator for dimension {i+1} ({'x' if i==0 else 'y' if i==1 else 'z'}):\")\n",
    "    for param, value in estimator.get_params().items():\n",
    "        print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"\\nOverall Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate and print MSE for each coordinate\n",
    "for i, coord in enumerate(['x', 'y', 'z']):\n",
    "    mse = mean_squared_error(y_test.iloc[:, i], y_pred[:, i])\n",
    "    print(f\"MSE for {coord}: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = np.mean([estimator.feature_importances_ for estimator in best_model.estimators_], axis=0)\n",
    "feature_importance_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "for i, estimator in enumerate(best_model.estimators_):\n",
    "    estimator.save_model(f'xgboost_ap_position_model_all_{i}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_ap_positions(y_true, y_pred):\n",
    "    # Calculate MSE for each point\n",
    "    mse = np.mean((y_true - y_pred)**2, axis=1)\n",
    "    \n",
    "    # Create the 3D plot\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot ground truth positions in blue\n",
    "    ax.scatter(y_true[:, 0], y_true[:, 1], y_true[:, 2], \n",
    "               c='green', marker='o', s=100, alpha=1, label='Ground Truth')\n",
    "    \n",
    "    norm = Normalize(vmin=0, vmax=10)\n",
    "    # Plot predicted positions with color based on MSE\n",
    "    scatter = ax.scatter(y_pred[:, 0], y_pred[:, 1], y_pred[:, 2],\n",
    "                         c=mse, norm=norm, cmap='Reds_r', s=30, alpha=0.7, label='Predicted')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(scatter)\n",
    "    cbar.set_label('Mean Squared Error')\n",
    "    \n",
    "    # Calculate overall MSE\n",
    "    overall_mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    ax.set_xlabel('X Position')\n",
    "    ax.set_ylabel('Y Position')\n",
    "    ax.set_zlabel('Z Position')\n",
    "    ax.legend()\n",
    "    plt.title(f'3D AP Positions - Overall MSE: {overall_mse:.4f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_ap_positions(y_test.to_numpy(), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sionna_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
